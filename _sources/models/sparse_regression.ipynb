{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c879e16",
   "metadata": {},
   "source": [
    "# Sparse regression\n",
    "\n",
    "In this example we will use a sparse binary regression with hierarchies on the scale of the independent variableâ€™s parameters that function as a proxy for variable selection. We will use the Horseshoe prior to {cite:p}`carvalho2010horseshoe` to ensure sparsity.\n",
    "\n",
    "The Horseshoe prior consists in putting a prior on the scale of the regression parameter $\\beta$: the product of a global $\\tau$ and local $\\lambda$ parameter that are both concentrated at $0$, thus allowing the corresponding regression parameter to degenerate at $0$ and effectively excluding this parameter from the model. This kind of model is challenging for samplers: the prior on $\\beta$'s scale parameter creates funnel geometries that are hard to efficiently explore {cite:p}`papaspiliopoulos2007general`.\n",
    "\n",
    "Mathematically, we will consider the following model:\n",
    "\n",
    "\\begin{align*}\n",
    "\\tau &\\sim \\operatorname{C}^+(0, 1)\\\\\n",
    "\\boldsymbol{\\lambda} &\\sim \\operatorname{C}^+(0, 1)\\\\\n",
    "\\boldsymbol{\\beta} &\\sim \\operatorname{Normal}(0, \\tau \\lambda)\\\\\n",
    "\\\\\n",
    "p &= \\operatorname{sigmoid}\\left(- X.\\boldsymbol{\\beta}\\right)\\\\\n",
    "y &\\sim \\operatorname{Bernoulli}(p)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The model is run on its *non-centered parametrization* {cite:p}`papaspiliopoulos2007general` with data from the numerical version of the German credit dataset. The target posterior is defined by its likelihood. We implement the model using [Aesara](https://github.com/aesara-devs/aesara):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265559d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aesara.tensor as at\n",
    "\n",
    "X_at = at.matrix('X')\n",
    "\n",
    "srng = at.random.RandomStream(0)\n",
    "\n",
    "tau_rv = srng.halfcauchy(0, 1)\n",
    "lambda_rv = srng.halfcauchy(0, 1, size=X_at.shape[-1])\n",
    "\n",
    "sigma = tau_rv * lambda_rv\n",
    "beta_rv = srng.normal(0, sigma, size=X_at.shape[-1])\n",
    "\n",
    "eta = X_at @ beta_rv\n",
    "p = at.sigmoid(-eta)\n",
    "Y_rv = srng.bernoulli(p, name=\"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008cc89e",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The non-centered parametrization is not necessarily adapted to every geometry. One should always check *a posteriori* the sampler did not encounter any funnel geomtry.\n",
    "```\n",
    "\n",
    "## German credit dataset\n",
    "\n",
    "We will use the sparse regression model on the German credit dataset {cite:p}`dua2017machine`. We use the numeric version that is adapted to models that cannot handle categorical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fffeb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_table(\n",
    "  \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data-numeric\",\n",
    "  header=None,\n",
    "  delim_whitespace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f08ca",
   "metadata": {},
   "source": [
    "Each row in the dataset corresponds to a different customer. The dependent variable $y$ is equal to $1$ when the customer has good credit and $2$ when it has bad credit; we encode it so a customer with good credit corresponds to $1$, a customer with bad credit $1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = -1 * (data.iloc[:, -1].values - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de05e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_bad = len(y[y==0.]) / len(y)\n",
    "r_good = len(y[y>1]) /  len(y)\n",
    "\n",
    "print(f\"{r_bad*100}% of the customers in the dataset are classified as having bad credit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81239057",
   "metadata": {},
   "source": [
    "The regressors are defined on different scales so we normalize their values, and add a column of $1$ that corresponds to the intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a8e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = (\n",
    "    data.iloc[:, :-1]\n",
    "    .apply(lambda x: -1 + (x - x.min()) * 2 / (x.max() - x.min()), axis=0)\n",
    "    .values\n",
    ")\n",
    "X = np.concatenate([np.ones((1000, 1)), X], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7d8961",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We generate a function that computes the model's logdensity using [AePPL](https://github.com/aesara-devs/aeppl). We transform the values of $\\tau$ and $\\lambda$ so the sampler can operate on variables defined on the real line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f918012",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "import aesara\n",
    "import aeppl\n",
    "from aeppl.transforms import TransformValuesRewrite, LogTransform\n",
    "\n",
    "transforms_op = TransformValuesRewrite(\n",
    "     {lambda_rv: LogTransform(), tau_rv: LogTransform()}\n",
    ")\n",
    "\n",
    "logdensity, value_variables = aeppl.joint_logprob(\n",
    "    tau_rv,\n",
    "    lambda_rv,\n",
    "    beta_rv,\n",
    "    realized={Y_rv: at.as_tensor(y)},\n",
    "    extra_rewrites=transforms_op\n",
    ")\n",
    "\n",
    "\n",
    "logdensity_aesara_fn = aesara.function([X_at] + list(value_variables), logdensity, mode=\"JAX\")\n",
    "\n",
    "def logdensity_fn(x):\n",
    "    tau = x['log_tau']\n",
    "    lmbda = x['log_lmbda']\n",
    "    beta = x['beta']\n",
    "    return logdensity_aesara_fn.vm.jit_fn(X, tau, lmbda, beta)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb93d0d",
   "metadata": {},
   "source": [
    "Let us now define a utility function that builds a sampling loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d540e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "def inference_loop(rng_key, init_state, kernel, n_iter):\n",
    "    keys = jax.random.split(rng_key, n_iter)\n",
    "\n",
    "    def step(state, key):\n",
    "        state, info = kernel(key, state)\n",
    "        return state, (state, info)\n",
    "\n",
    "    _, (states, info) = jax.lax.scan(step, init_state, keys)\n",
    "    return states, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578c4aad",
   "metadata": {},
   "source": [
    "### MEADS\n",
    "\n",
    "The MEADS algorithm {cite:p}`hoffman2022tuning` is a combination of Generalized HMC with a parameter tuning procedure. Let us initialize the position of the chain first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chains = 128\n",
    "num_warmup = 2000\n",
    "num_samples = 2000\n",
    "\n",
    "rng_key = jax.random.PRNGKey(10)\n",
    "rng_key, key_b, key_l, key_t = jax.random.split(rng_key, 4)\n",
    "init_position = {\n",
    "    \"beta\": jax.random.normal(key_b, (num_chains, X.shape[1])),\n",
    "    \"log_lmbda\": jax.random.normal(key_l, (num_chains, X.shape[1])),\n",
    "    \"log_tau\": jax.random.normal(key_t, (num_chains,)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ce096",
   "metadata": {},
   "source": [
    "Here we will not use the adaptive version of the MEADS algorithm, but instead use their heuristics as an adaptation procedure for Generalized Hamiltonian Monte Carlo kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b5896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import blackjax\n",
    "\n",
    "key_warmup, key_sample = jax.random.split(rng_key)\n",
    "meads = blackjax.meads_adaptation(logdensity_fn, num_chains)\n",
    "(state, parameters), _ = meads.run(key_warmup, init_position, num_warmup)\n",
    "kernel = blackjax.ghmc(logdensity_fn, **parameters).step\n",
    "\n",
    "# Choose the last state of the first two chains as a starting point for the sampler\n",
    "init_states = jax.tree_util.tree_map(lambda x: x[:2], state)\n",
    "keys = jax.random.split(rng_key, 2)\n",
    "samples, info = jax.vmap(inference_loop, in_axes=(0, 0, None, None))(keys, init_states, kernel, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2614db0b",
   "metadata": {},
   "source": [
    "Let us look a high-level summary statistics for the inference, including the split-Rhat value and the number of effective samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.diagnostics import print_summary\n",
    "\n",
    "print_summary(samples.position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b11bb3",
   "metadata": {},
   "source": [
    "Let's check if there are any divergent transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d74b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(info.is_divergent, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b0a4a9",
   "metadata": {},
   "source": [
    "We warned earlier that the non-centered parametrization was not a one-size-fits-all solution to the funnel geometries that can be present in the posterior distribution. Although there was no divergence, it is still worth checking the posterior interactions between the coefficients to make sure the posterior geometry did not get in the way of sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b6c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pylab as plt\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "gs = gridspec.GridSpec(10, 3)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 18))\n",
    "x = np.linspace(-2., 2., 100)\n",
    "for i in range(20):\n",
    "    ax = fig.add_subplot(gs[i%10, i//10])\n",
    "    ax.plot(samples.position[\"log_lmbda\"][0,:,i], samples.position[\"beta\"][0,:,i], 'o', ms=.4)\n",
    "    ax.set_xlabel(rf\"$\\lambda$[{i}]\")\n",
    "    ax.set_ylabel(rf\"$\\beta$[{i}]\")\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a71f4c",
   "metadata": {},
   "source": [
    "While some parameters (for instance the 15th) exhibit no particular correlations, the funnel geometry can still be observed for a few of them (4th, 13th, etc.). Ideally one would adopt a centered parametrization for those parameters to get a better approximation to the true posterior distribution, but here we also assess the ability of the sampler to explore these funnel geometries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66d0529",
   "metadata": {},
   "source": [
    "We can convince ourselves that the Horseshoe prior induces sparsity on the regression coefficients by looking at their posterior distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pylab as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "gs = gridspec.GridSpec(10, 3)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 18))\n",
    "x = np.linspace(-2., 2., 100)\n",
    "for i in range(20):\n",
    "    ax = fig.add_subplot(gs[i%10, i//10])\n",
    "    ax.hist(np.array(samples.position[\"beta\"][0, :, i]), bins=20)\n",
    "    ax.set_xlabel(rf\"$\\beta$[{i}]\")\n",
    "\n",
    "    ax.set_xlim([-2, 2])\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8bdb7f",
   "metadata": {},
   "source": [
    "Indeed, many of the parameters are centered around $0$.\n",
    "\n",
    "```{note}\n",
    "It is interesting to notice that the interactions for the parameters with large values do not exhibit funnel geometries.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb8a50b",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.13.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   13,
   34,
   50,
   60,
   68,
   72,
   76,
   81,
   85,
   94,
   100,
   127,
   131,
   143,
   149,
   161,
   165,
   177,
   181,
   185,
   189,
   191,
   195,
   213,
   217,
   221,
   240,
   248
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}