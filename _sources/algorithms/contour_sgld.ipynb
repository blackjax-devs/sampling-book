{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a695b673",
   "metadata": {},
   "source": [
    "# Contour stochastic gradient Langevin dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff1c6dd",
   "metadata": {},
   "source": [
    "Sampling in big data problems is fundamentally limited by the multi-modality of the target distributions, with extremely high energy barriers. Multi-modality is often empirically solved via cyclical learning rates or different initializations (parallel chains).\n",
    "\n",
    "Contour SgLD takes a different approach altogether: the algorithms learns the energy landscape with sampling, and uses this approximation to effectively integrate the diffusion on a flat landscape, before using the importance weight to reweigh the obtained samples.\n",
    "\n",
    "In this notebook we will compare the performance of SGLD and Contour SGLD on a simple bimodal gaussian target. This example looks simple, but is rather challenging to sample with most methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d832dc7",
   "metadata": {},
   "source": [
    "## Gaussian Mixture model\n",
    "\n",
    "Let us first generate data points that follow a gaussian mixture distributions. The example appears simple, and yet it is hard enough for most algorithms to fail to recover the two modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9b4633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "\n",
    "\n",
    "def gaussian_mixture_model(mu=-5.0, sigma=5.0, gamma=20.0):\n",
    "    def sample_fn(rng_key, num_samples):\n",
    "        key1, key2, key3 = jax.random.split(rng_key, 3)\n",
    "        prob_mixture = jax.random.bernoulli(key1, p=0.5, shape=(num_samples, 1))\n",
    "        mixture_1 = jax.random.normal(key2, shape=(num_samples, 1)) * sigma + mu\n",
    "        mixture_2 = jax.random.normal(key3, shape=(num_samples, 1)) * sigma + gamma - mu\n",
    "        return prob_mixture * mixture_1 + (1 - prob_mixture) * mixture_2\n",
    "\n",
    "    def logprior_fn(position):\n",
    "        return 0\n",
    "\n",
    "    def loglikelihood_fn(position, x):\n",
    "        mixture_1 = jax.scipy.stats.norm.logpdf(x, loc=position, scale=sigma)\n",
    "        mixture_2 = jax.scipy.stats.norm.logpdf(x, loc=-position + gamma, scale=sigma)\n",
    "        return jsp.special.logsumexp(jnp.array([mixture_1, mixture_2])) + jnp.log(0.5)\n",
    "\n",
    "    return sample_fn, logprior_fn, loglikelihood_fn\n",
    "\n",
    "\n",
    "sample_fn, logprior_fn, loglikelihood_fn = gaussian_mixture_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1bfe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 1000\n",
    "\n",
    "rng_key = jax.random.PRNGKey(888)\n",
    "rng_key, sample_key = jax.random.split(rng_key)\n",
    "X_data = sample_fn(sample_key, data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e400eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "ax.hist(X_data.squeeze(), 100)\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_xlim(left=-15, right=35)\n",
    "\n",
    "ax.set_yticks([])\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "\n",
    "plt.title(\"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b8e70a",
   "metadata": {},
   "source": [
    "## Sample with Contour SGLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastprogress import progress_bar\n",
    "\n",
    "import blackjax\n",
    "import blackjax.sgmcmc.gradients as gradients\n",
    "\n",
    "# Specify hyperparameters for SGLD\n",
    "total_iter = 10_000\n",
    "thinning_factor = 10\n",
    "\n",
    "batch_size = 100\n",
    "lr = 1e-3\n",
    "temperature = 50.0\n",
    "\n",
    "init_position = 10.0\n",
    "\n",
    "\n",
    "# Build the SGDL sampler\n",
    "grad_fn = gradients.grad_estimator(logprior_fn, loglikelihood_fn, data_size)\n",
    "sgld = blackjax.sgld(grad_fn)\n",
    "\n",
    "\n",
    "# Initialize and take one step using the vanilla SGLD algorithm\n",
    "position = init_position\n",
    "sgld_sample_list = jnp.array([])\n",
    "\n",
    "pb = progress_bar(range(total_iter))\n",
    "for iter_ in pb:\n",
    "    rng_key, batch_key, sample_key = jax.random.split(rng_key, 3)\n",
    "    data_batch = jax.random.shuffle(batch_key, X_data)[:batch_size, :]\n",
    "    position = jax.jit(sgld)(sample_key, position, data_batch, lr, temperature)\n",
    "    if iter_ % thinning_factor == 0:\n",
    "        sgld_sample_list = jnp.append(sgld_sample_list, position)\n",
    "        pb.comment = f\"| position: {position: .2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ccdeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "G = gridspec.GridSpec(1, 3)\n",
    "\n",
    "# Trajectory\n",
    "ax = plt.subplot(G[0, :2])\n",
    "ax.plot(sgld_sample_list, label=\"SGLD\")\n",
    "ax.set_xlabel(f\"Iterations (x{thinning_factor})\")\n",
    "ax.set_ylabel(\"X\")\n",
    "\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "\n",
    "# Histogram\n",
    "ax = plt.subplot(G[0, 2])\n",
    "ax.hist(sgld_sample_list, 100)\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_xlim(left=-15, right=35)\n",
    "\n",
    "ax.set_yticks([])\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "\n",
    "\n",
    "plt.suptitle(\"Stochastic gradient Langevin dynamics (SGLD)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify hyperparameters (zeta and sz are the only two hyperparameters to tune)\n",
    "zeta = 2\n",
    "sz = 10\n",
    "temperature = 50\n",
    "\n",
    "lr = 1e-3\n",
    "init_position = 10.0\n",
    "\n",
    "\n",
    "# The following parameters partition the energy space and no tuning is needed.\n",
    "num_partitions = 100000\n",
    "energy_gap = 0.25\n",
    "domain_radius = 50  # restart sampling when the particle explores too deep over the tails and leads to nan.\n",
    "\n",
    "\n",
    "logdensity_fn = gradients.logdensity_estimator(logprior_fn, loglikelihood_fn, data_size)\n",
    "csgld = blackjax.csgld(\n",
    "    logdensity_fn,\n",
    "    zeta=zeta,  # can be specified at each step in lower-level interface\n",
    "    temperature=temperature,  # can be specified at each step\n",
    "    num_partitions=num_partitions,  # cannot be specified at each step\n",
    "    energy_gap=energy_gap,  # cannot be specified at each step\n",
    "    min_energy=0,\n",
    ")\n",
    "\n",
    "# 3.1 Simulate via the CSGLD algorithm\n",
    "state = csgld.init(init_position)\n",
    "\n",
    "csgld_sample_list, csgld_energy_idx_list = jnp.array([]), jnp.array([])\n",
    "\n",
    "pb = progress_bar(range(total_iter))\n",
    "for iter_ in pb:\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    stepsize_SA = min(1e-2, (iter_ + 100) ** (-0.8)) * sz\n",
    "\n",
    "    data_batch = jax.random.shuffle(rng_key, X_data)[:batch_size, :]\n",
    "    state = jax.jit(csgld.step)(subkey, state, data_batch, lr, stepsize_SA)\n",
    "\n",
    "    if iter_ % thinning_factor == 0:\n",
    "        csgld_sample_list = jnp.append(csgld_sample_list, state.position)\n",
    "        csgld_energy_idx_list = jnp.append(csgld_energy_idx_list, state.energy_idx)\n",
    "        pb.comment = (\n",
    "            f\"| position {state.position: .2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecad2fc",
   "metadata": {},
   "source": [
    "Contour SGLD is a meta-algorithm, based on Stochastic Gradient Langevin Dynamics. It takes inspiration from the Wang-Landau algorithm to learn the density of states of the model at each energy level, and uses this information to \"flatten\" the target density so the sampler can explore it more easily.\n",
    "\n",
    "As a result, the samples returned by contour SGLD are not from the target density directly, and we need to resample them using the density of state as importance weights to get samples from the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91281c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_idx = jnp.where(state.energy_pdf > jnp.quantile(state.energy_pdf, 0.95))[0]\n",
    "scaled_energy_pdf = (\n",
    "    state.energy_pdf[important_idx] ** zeta\n",
    "    / (state.energy_pdf[important_idx] ** zeta).max()\n",
    ")\n",
    "\n",
    "csgld_re_sample_list = jnp.array([])\n",
    "for _ in range(5):\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    for my_idx in important_idx:\n",
    "        if jax.random.bernoulli(rng_key, p=scaled_energy_pdf[my_idx], shape=None) == 1:\n",
    "            samples_in_my_idx = csgld_sample_list[csgld_energy_idx_list == my_idx]\n",
    "            csgld_re_sample_list = jnp.concatenate(\n",
    "                (csgld_re_sample_list, samples_in_my_idx)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "G = gridspec.GridSpec(1, 3)\n",
    "\n",
    "# Trajectory\n",
    "ax = plt.subplot(G[0, :2])\n",
    "ax.plot(csgld_sample_list, label=\"SGLD\")\n",
    "ax.set_xlabel(f\"Iterations (x{thinning_factor})\")\n",
    "ax.set_ylabel(\"X\")\n",
    "\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "\n",
    "# Histogram before resampling\n",
    "ax = plt.subplot(G[0, 2])\n",
    "ax.hist(csgld_sample_list, 100, label=\"before resampling\")\n",
    "ax.hist(csgld_re_sample_list, 100, label=\"after resampling\")\n",
    "\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_xlim(left=-15, right=35)\n",
    "\n",
    "ax.set_yticks([])\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "\n",
    "plt.legend()\n",
    "plt.suptitle(\"Contour SGLD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25730724",
   "metadata": {},
   "source": [
    "## How does Contour SGLD work?\n",
    "\n",
    "The energy density is crucial for us to build a flat density, so let's take a look at the estimation returned by the algorithm. For illustration purposes, we smooth out fluctations and focus on the energy range from 3700 to 100000, which covers the major part of sample space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175347d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_energy_pdf = jnp.convolve(\n",
    "    state.energy_pdf, jsp.stats.norm.pdf(jnp.arange(-100, 101), scale=10), mode=\"same\"\n",
    ")\n",
    "interested_idx = jax.lax.floor((jnp.arange(3700, 10000)) / energy_gap).astype(\n",
    "    \"int32\"\n",
    ")  # min 3681\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(\n",
    "    jnp.arange(num_partitions)[interested_idx] * energy_gap,\n",
    "    smooth_energy_pdf[interested_idx],\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Energy\")\n",
    "ax.set_ylabel(\"Energy Density\")\n",
    "\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b80b03e",
   "metadata": {},
   "source": [
    "From the figure above, we see that low-energy regions usually lead to much higher probability mass. Moreover, the slope is negative with a higher scale in low energy regions. In view of Eq.(8) in [the paper]( https://proceedings.neurips.cc/paper/2020/file/b5b8c484824d8a06f4f3d570bc420313-Paper.pdf), we can expect a **negative learning rate** to help the particle escape the local trap. Eventually, a particle is able to bounce out of the deep local traps freely instead of being absorbed into it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a304d1c",
   "metadata": {},
   "source": [
    "Admittedly, this algorithm is a little sophisticated due to the need to partition the energy space; Learning energy pdf also makes this algorithm delicate and leads to a large variance. However, this allows to escape deep local traps in a principled sampling framework without using any tricks (cyclical learning rates or different initializations). The variance-reduced version is studied in [this work](https://arxiv.org/pdf/2202.09867.pdf)."
   ]
  }
 ],
 "metadata": {
  "file_format": "mystnb",
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.14.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "mystnb": {
   "execution_timeout": 200
  },
  "source_map": [
   15,
   19,
   27,
   33,
   61,
   69,
   83,
   87,
   123,
   156,
   201,
   207,
   225,
   258,
   264,
   286,
   290
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}